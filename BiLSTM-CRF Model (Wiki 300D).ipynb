{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Afrikaans BiLSTM-CRF Model (Wiki 300D) ",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deanhoperobertson/Afrikaans-Named-Entity-Recognition/blob/master/BiLSTM-CRF%20Model%20(Wiki%20300D).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1D_csm308YL",
        "colab_type": "text"
      },
      "source": [
        "# Afrikaans Bi-LSTM-CRF Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xpaxxz_RQCyY",
        "colab_type": "code",
        "outputId": "948f3ea0-2e3c-48f3-ffce-2e23dc84ff98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        }
      },
      "source": [
        "!sudo pip install git+https://www.github.com/keras-team/keras-contrib.git\n",
        "!pip install sklearn_crfsuite\n",
        "!pip install keras==2.2.4"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://www.github.com/keras-team/keras-contrib.git\n",
            "  Cloning https://www.github.com/keras-team/keras-contrib.git to /tmp/pip-req-build-tx2a00id\n",
            "  Running command git clone -q https://www.github.com/keras-team/keras-contrib.git /tmp/pip-req-build-tx2a00id\n",
            "Requirement already satisfied (use --upgrade to upgrade): keras-contrib==2.0.8 from git+https://www.github.com/keras-team/keras-contrib.git in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (from keras-contrib==2.0.8) (2.2.4)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.4.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.12.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (2.8.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (3.13)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras->keras-contrib==2.0.8) (1.17.5)\n",
            "Building wheels for collected packages: keras-contrib\n",
            "  Building wheel for keras-contrib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-contrib: filename=keras_contrib-2.0.8-cp36-none-any.whl size=101065 sha256=8ead72d51e61dcc24f972922796f4c8f42b317539bd658c842f3ba0154212360\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-r2zjf1mn/wheels/11/27/c8/4ed56de7b55f4f61244e2dc6ef3cdbaff2692527a2ce6502ba\n",
            "Successfully built keras-contrib\n",
            "Requirement already satisfied: sklearn_crfsuite in /usr/local/lib/python3.6/dist-packages (0.3.6)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.6/dist-packages (from sklearn_crfsuite) (4.28.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sklearn_crfsuite) (1.12.0)\n",
            "Requirement already satisfied: python-crfsuite>=0.8.3 in /usr/local/lib/python3.6/dist-packages (from sklearn_crfsuite) (0.9.6)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.6/dist-packages (from sklearn_crfsuite) (0.8.6)\n",
            "Requirement already satisfied: keras==2.2.4 in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.17.5)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.0.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.12.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (2.8.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.4.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras==2.2.4) (1.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcyQUZOR1EFa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import urllib.request\n",
        "\n",
        "#keras and tensorflow packages\n",
        "from keras.layers.merge import add\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing import text, sequence\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Model, Input\n",
        "from keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional,concatenate\n",
        "from keras_contrib.layers import CRF\n",
        "from keras_contrib.losses import crf_loss\n",
        "from keras_contrib.metrics import crf_viterbi_accuracy\n",
        "from keras_contrib.metrics import crf_accuracy\n",
        "\n",
        "\n",
        "#evaluation\n",
        "from sklearn_crfsuite.metrics import flat_classification_report,flat_f1_score,flat_precision_score\n",
        "from sklearn.model_selection import train_test_split\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgpmSzX_BTcF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def hasNumbers(text):\n",
        "    if text.isdigit():\n",
        "        return \"1\"\n",
        "    elif re.search(r'\\d',text) and re.search(r'\\,|\\.',text):\n",
        "        return \"1\" \n",
        "    else:\n",
        "        if re.search(r'\\d', text):\n",
        "            return(re.sub('\\d','D', text))\n",
        "        else:\n",
        "            return text\n",
        "\n",
        "def readstring(filename, meth):\n",
        "    f = filename.split('\\n')\n",
        "    sentences = []\n",
        "    sentence = []\n",
        "    for line in f:\n",
        "        if line == '\\r' or line.startswith('-DOCSTART') or line  ==\"\":\n",
        "            if len(sentence) > 0:\n",
        "                sentences.append(sentence)\n",
        "                sentence = []\n",
        "            continue\n",
        "        splits = line.split('\\t')\n",
        "        if meth.lower()==\"numbers\":\n",
        "          sentence.append([hasNumbers(splits[0]), splits[-1].strip()])\n",
        "        else:\n",
        "          sentence.append([splits[0], splits[-1].strip('\\r')])\n",
        "\n",
        "    if len(sentence) > 0:\n",
        "        sentences.append(sentence)\n",
        "        sentence = []\n",
        "    return sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdSHMu4l1M1N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import data from my github repo\n",
        "data_url = \"https://raw.githubusercontent.com/deanhoperobertson/Named-Enitty-Recognition/master/Data/Afrikaans/Train.txt\"\n",
        "data = urllib.request.urlopen(data_url).read()\n",
        "data = data.decode('utf-8')\n",
        "\n",
        "# #preproces the txt file\n",
        "data = readstring(data,\"Numbers\")\n",
        "\n",
        "train_data,test_data=train_test_split(data,train_size=.8, random_state=0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7Cyc1pHH8-n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reformat_data(data,meth):\n",
        "  if meth.lower() == \"data\":\n",
        "    i=0\n",
        "  else: i=1\n",
        "  train = []\n",
        "  output= []\n",
        "  for sentence in data:\n",
        "    words=[]\n",
        "    for x in sentence:\n",
        "      words.append(x[i])\n",
        "    train.append(words)\n",
        "\n",
        "  for i in train:\n",
        "    string = ' '.join(i)\n",
        "    output.append(string)\n",
        "  return output\n",
        "\n",
        "def get_max_length(corpus):\n",
        "  length = []\n",
        "  for sentence in corpus:\n",
        "    length.append(len(sentence))\n",
        "  return int(max(length))\n",
        "\n",
        "def number_of_tags(corpus):\n",
        "  tags=[]\n",
        "  for sentence in corpus:\n",
        "    for tag in sentence:\n",
        "      tags.append(tag[1])\n",
        "  return int(len(list(set(tags))))\n",
        "\n",
        "\n",
        "MAX_LEN = get_max_length(data)\n",
        "N_tags = number_of_tags(data)\n",
        "\n",
        "train = reformat_data(train_data,\"data\")\n",
        "test = reformat_data(test_data,\"data\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fy2I5XDg1NlV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create a tokenizer\n",
        "token_word = text.Tokenizer(char_level=False, lower=True, filters=\"}\", oov_token='UNK')\n",
        "token_word.fit_on_texts(train)\n",
        "\n",
        "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
        "X_train = sequence.pad_sequences(token_word.texts_to_sequences(train), maxlen=MAX_LEN,padding=\"post\")\n",
        "X_test = sequence.pad_sequences(token_word.texts_to_sequences(test), maxlen=MAX_LEN,padding=\"post\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V52szKi11hZp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train = reformat_data(train_data,\"tags\")\n",
        "test = reformat_data(test_data,\"tags\")\n",
        "\n",
        "# create a tokenizer\n",
        "token_tag = text.Tokenizer(char_level=False, lower=False, filters=\"}\")\n",
        "token_tag.fit_on_texts(train)\n",
        "\n",
        "# convert text to sequence of tokens and pad them to ensure equal length vectors \n",
        "Y = sequence.pad_sequences(token_tag.texts_to_sequences(train), maxlen=MAX_LEN,padding=\"post\")\n",
        "# One-Hot encode categorical labels\n",
        "Y_train = [to_categorical(i, num_classes=N_tags+1) for i in Y]\n",
        "\n",
        "Y = sequence.pad_sequences(token_tag.texts_to_sequences(test), maxlen=MAX_LEN,padding=\"post\")\n",
        "# One-Hot encode categorical labels\n",
        "Y_test = [to_categorical(i, num_classes=N_tags+1) for i in Y]\n",
        "\n",
        "#add padding \n",
        "token_tag.index_word[0]=\"PAD\"\n",
        "sub_label = list(token_tag.index_word.values())\n",
        "sub_label.remove('OUT')\n",
        "sub_label.remove('PAD')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vcnym_iQ9Alp",
        "colab_type": "text"
      },
      "source": [
        "## Adding Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZaQyYvqVUNX",
        "colab_type": "code",
        "outputId": "dbeb2027-b6b9-4db7-bda6-bf89317db8ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lWR33m2GsYK",
        "colab_type": "code",
        "outputId": "b737f460-2c6b-41ee-f030-6c94a4c03343",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "#root_path = \"/content/drive/My Drive/wiki.af.300.vec\"\n",
        "root_path = \"/content/drive/My Drive/wiki.af.vec\"\n",
        "\n",
        "EMBEDDING=300\n",
        "embeddings_index={}\n",
        "f = open(root_path, encoding = \"utf-8\")\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = ''.join(values[:-EMBEDDING])\n",
        "    coefs = np.asarray(values[-EMBEDDING:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 95882 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJewq4IPQODD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create emedding matrix\n",
        "word_index = token_word.word_index\n",
        "embedding_matrix = np.zeros((len(token_word.word_index) + 1, EMBEDDING))\n",
        "lower =0\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is None:\n",
        "      embedding_vector = embeddings_index.get(word.lower)\n",
        "      if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "        lower +=1\n",
        "      else: \n",
        "        pass\n",
        "    else:\n",
        "      embedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxhWA-20bziN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d822652b-8238-4639-9851-face6b74d40e"
      },
      "source": [
        "checks=[]\n",
        "words=[]\n",
        "for i in range(0,len(token_word.word_index)+1):\n",
        "  if embedding_matrix[i][0] == 0.0:\n",
        "    checks.append(1)\n",
        "    words.append(list(token_word.word_index.items())[i-1][0])\n",
        "  else:\n",
        "    continue\n",
        "\n",
        "print(\"Missing words from Embeddings: %d (%.2f%%) \\nCase Changed to find: %d\" %(len(checks),(len(checks)/len(token_word.word_index)*100),lower))"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Missing words from Embeddings: 7004 (39.69%) \n",
            "Case Changed to find: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ic_YAE0DIhNp",
        "colab_type": "text"
      },
      "source": [
        "# Create Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Foc0qe4C2Prp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model definition\n",
        "\n",
        "input = Input(shape=(MAX_LEN,))\n",
        "\n",
        "model_2 = Embedding(len(token_word.word_index) + 1,output_dim=EMBEDDING,\n",
        "                  weights=[embedding_matrix],input_length=MAX_LEN,\n",
        "                  trainable=False,mask_zero=True)(input)\n",
        "\n",
        "model_2 = Bidirectional(LSTM(units=300, return_sequences=True,\n",
        "                           recurrent_dropout=0.1, dropout=0.2))(model_2)\n",
        "\n",
        "model_2 = Bidirectional(LSTM(units=300, return_sequences=True,\n",
        "                           recurrent_dropout=0.1, dropout=0.2))(model_2)\n",
        "\n",
        "model_2 = TimeDistributed(Dense(50, activation=\"relu\"))(model_2)\n",
        "crf = CRF(N_tags+1)  # CRF layer\n",
        "out = crf(model_2)  # output\n",
        "model_2 = Model(input, out)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CJIyeYl5VAx",
        "colab_type": "code",
        "outputId": "0f3c358c-0096-4d26-df76-2545343563d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "model_2.compile(optimizer='adam', loss=crf_loss,metrics=[crf_viterbi_accuracy])\n",
        "model_2.summary()"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 396)               0         \n",
            "_________________________________________________________________\n",
            "embedding_2 (Embedding)      (None, 396, 300)          5294700   \n",
            "_________________________________________________________________\n",
            "bidirectional_3 (Bidirection (None, 396, 600)          1442400   \n",
            "_________________________________________________________________\n",
            "bidirectional_4 (Bidirection (None, 396, 600)          2162400   \n",
            "_________________________________________________________________\n",
            "time_distributed_2 (TimeDist (None, 396, 50)           30050     \n",
            "_________________________________________________________________\n",
            "crf_2 (CRF)                  (None, 396, 10)           630       \n",
            "=================================================================\n",
            "Total params: 8,930,180\n",
            "Trainable params: 3,635,480\n",
            "Non-trainable params: 5,294,700\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXBfkQuB5dDP",
        "colab_type": "code",
        "outputId": "9f57eabd-3713-4ba1-e8f9-72d072334b07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "%%time\n",
        "BATCH_SIZE = 200\n",
        "EPOCHS=10\n",
        "from keras.callbacks import EarlyStopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=2)\n",
        "history = model_2.fit(X_train, np.array(Y_train), batch_size=BATCH_SIZE, epochs=EPOCHS, validation_split=0.2, verbose=1,callbacks=[early_stopping])"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Train on 5734 samples, validate on 1434 samples\n",
            "Epoch 1/10\n",
            "5734/5734 [==============================] - 179s 31ms/step - loss: 53.4607 - crf_viterbi_accuracy: 0.8447 - val_loss: 52.6562 - val_crf_viterbi_accuracy: 0.8827\n",
            "Epoch 2/10\n",
            "5734/5734 [==============================] - 172s 30ms/step - loss: 53.1794 - crf_viterbi_accuracy: 0.8888 - val_loss: 52.4997 - val_crf_viterbi_accuracy: 0.8877\n",
            "Epoch 3/10\n",
            "5734/5734 [==============================] - 173s 30ms/step - loss: 53.0532 - crf_viterbi_accuracy: 0.9028 - val_loss: 52.4035 - val_crf_viterbi_accuracy: 0.9100\n",
            "Epoch 4/10\n",
            "5734/5734 [==============================] - 173s 30ms/step - loss: 52.9912 - crf_viterbi_accuracy: 0.9157 - val_loss: 52.3692 - val_crf_viterbi_accuracy: 0.9159\n",
            "Epoch 5/10\n",
            "5734/5734 [==============================] - 173s 30ms/step - loss: 52.9543 - crf_viterbi_accuracy: 0.9234 - val_loss: 52.3333 - val_crf_viterbi_accuracy: 0.9263\n",
            "Epoch 6/10\n",
            "5734/5734 [==============================] - 172s 30ms/step - loss: 52.9279 - crf_viterbi_accuracy: 0.9279 - val_loss: 52.3166 - val_crf_viterbi_accuracy: 0.9306\n",
            "Epoch 7/10\n",
            "5734/5734 [==============================] - 172s 30ms/step - loss: 52.9064 - crf_viterbi_accuracy: 0.9330 - val_loss: 52.2970 - val_crf_viterbi_accuracy: 0.9349\n",
            "Epoch 8/10\n",
            "5734/5734 [==============================] - 171s 30ms/step - loss: 52.8911 - crf_viterbi_accuracy: 0.9351 - val_loss: 52.2980 - val_crf_viterbi_accuracy: 0.9348\n",
            "Epoch 9/10\n",
            "5734/5734 [==============================] - 172s 30ms/step - loss: 52.8792 - crf_viterbi_accuracy: 0.9380 - val_loss: 52.2819 - val_crf_viterbi_accuracy: 0.9375\n",
            "Epoch 10/10\n",
            "5734/5734 [==============================] - 172s 30ms/step - loss: 52.8656 - crf_viterbi_accuracy: 0.9406 - val_loss: 52.2755 - val_crf_viterbi_accuracy: 0.9406\n",
            "CPU times: user 45min 11s, sys: 4min 50s, total: 50min 2s\n",
            "Wall time: 28min 52s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIqFQiEtFovF",
        "colab_type": "text"
      },
      "source": [
        "## Predict on Training Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzyp8RqJ5xmL",
        "colab_type": "code",
        "outputId": "f8542b85-d550-4fb8-de6c-7182da60037b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "%%time\n",
        "pred = model_2.predict(X_train, verbose=1)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7168/7168 [==============================] - 440s 61ms/step\n",
            "CPU times: user 11min 20s, sys: 1min 46s, total: 13min 7s\n",
            "Wall time: 7min 20s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIKABaGMH4rB",
        "colab_type": "code",
        "outputId": "452d5921-ccfa-490d-a59b-041e6761f17f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "# TRain Eval\n",
        "preds = np.argmax(pred, axis=-1)\n",
        "y_tr_true = np.argmax(Y_train, -1)\n",
        "\n",
        "# Convert the index to tag\n",
        "pred_tag = [[token_tag.index_word[i] for i in row] for row in preds]\n",
        "y_tr_true_tag = [[token_tag.index_word[i] for i in row] for row in y_tr_true]\n",
        "                 \n",
        "                 \n",
        "report = flat_classification_report(y_pred=pred_tag, y_true=y_tr_true_tag,labels=sub_label)\n",
        "print(report)\n",
        "#F1 Score\n",
        "score=flat_f1_score(y_pred=pred_tag, y_true=y_tr_true_tag,average='micro',labels=sub_label)\n",
        "print(score)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      I-MISC       0.78      0.71      0.74      5371\n",
            "      B-MISC       0.72      0.51      0.60      5264\n",
            "       B-ORG       0.72      0.58      0.64      2831\n",
            "       I-ORG       0.73      0.73      0.73      2164\n",
            "      I-PERS       0.84      0.78      0.81      1715\n",
            "      B-PERS       0.82      0.72      0.77      1627\n",
            "       B-LOC       0.77      0.60      0.67      1478\n",
            "       I-LOC       0.78      0.48      0.59       288\n",
            "\n",
            "   micro avg       0.76      0.64      0.70     20738\n",
            "   macro avg       0.77      0.64      0.70     20738\n",
            "weighted avg       0.76      0.64      0.69     20738\n",
            "\n",
            "0.6955521271866748\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QP9XMC6I-zp",
        "colab_type": "text"
      },
      "source": [
        "# Predict On Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFcDUNnKUs1u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "a2fc2734-aac1-422d-f965-0c5204f3d893"
      },
      "source": [
        "%%time\n",
        "pred = model_2.predict(X_test, verbose=1)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1793/1793 [==============================] - 112s 62ms/step\n",
            "CPU times: user 2min 53s, sys: 27 s, total: 3min 20s\n",
            "Wall time: 1min 51s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mbx8KiknJD0I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "8580e581-05ab-46ab-e75a-02591e8b18af"
      },
      "source": [
        "# Test Eval\n",
        "#pred_cat = model.predict(X_tr)\n",
        "preds = np.argmax(pred, axis=-1)\n",
        "y_tr_true = np.argmax(Y_test, -1)\n",
        "\n",
        "# Convert the index to tag\n",
        "pred_tag = [[token_tag.index_word[i] for i in row] for row in preds]\n",
        "y_tr_true_tag = [[token_tag.index_word[i] for i in row] for row in y_tr_true]\n",
        "                 \n",
        "                 \n",
        "report = flat_classification_report(y_pred=pred_tag, y_true=y_tr_true_tag, labels=sub_label)\n",
        "print(report)\n",
        "\n",
        "score=flat_f1_score(y_pred=pred_tag, y_true=y_tr_true_tag,average='micro', labels=sub_label)\n",
        "print(score)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      I-MISC       0.70      0.65      0.67      1357\n",
            "      B-MISC       0.64      0.47      0.54      1267\n",
            "       B-ORG       0.62      0.46      0.53       726\n",
            "       I-ORG       0.68      0.58      0.63       551\n",
            "      I-PERS       0.77      0.69      0.73       440\n",
            "      B-PERS       0.80      0.62      0.70       412\n",
            "       B-LOC       0.72      0.49      0.58       340\n",
            "       I-LOC       0.68      0.26      0.38        50\n",
            "\n",
            "   micro avg       0.69      0.56      0.62      5143\n",
            "   macro avg       0.70      0.53      0.59      5143\n",
            "weighted avg       0.69      0.56      0.61      5143\n",
            "\n",
            "0.6161616161616161\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujEAWjF1R499",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}